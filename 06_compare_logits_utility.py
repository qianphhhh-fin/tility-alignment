import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import gc
from tqdm import tqdm
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# ============================
# 1. å®éªŒé…ç½®
# ============================
BASE_MODEL_NAME = "Qwen/Qwen3-0.6B" # æœªç»SFTçš„åŸºåº§
SFT_MODEL_PATH = "sft-agent-0.6b"              # ç»è¿‡SFTçš„æ¨¡å‹
NUM_SAMPLES = 200                              # æµ‹è¯•æ ·æœ¬æ•° (è¶Šå¤šè¶Šå‡†)

# ç›®æ ‡äººæ ¼å‚æ•° (Ground Truth)
TARGET_ALPHA = 0.88
TARGET_LAMBDA = 2.25

# ============================
# 2. æ•°æ®ä¸å·¥å…·å‡½æ•°
# ============================
def calculate_utility(v):
    """çœŸå®æ•ˆç”¨è®¡ç®—å‡½æ•°"""
    if v >= 0: return v ** TARGET_ALPHA
    return -TARGET_LAMBDA * ((-v) ** TARGET_ALPHA)

def get_utility_diff(p1, v1, v2, sure):
    """è¿”å› U(Sure) - EU(Gamble)"""
    u_sure = calculate_utility(sure)
    u_gamble = (p1/100 * calculate_utility(v1)) + ((100-p1)/100 * calculate_utility(v2))
    return u_sure - u_gamble

def construct_perfect_context(tokenizer, p1, v1, v2, sure):
    p2 = 100 - p1
    
    # 1. è®¡ç®—æ­£ç¡®æ•°å€¼
    val_sure = calculate_utility(sure)
    val_gamble = (p1/100 * calculate_utility(v1)) + (p2/100 * calculate_utility(v2))
    
    decision_text = "accept" if val_sure > val_gamble else "reject"
    
    # 2. æ„é€  Prompt (ä¿æŒä¸å˜)
    system_prompt = f"You are a rational economic agent. Use the <tool> tag to perform python calculations. Finally output 'Final Decision: accept' or 'reject'."
    user_prompt = f"The prospect is: {p1}% chance of ${v1}, {p2}% chance of ${v2}. The sure outcome is: ${sure}. Do you accept?"
    
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    # 3. æ„é€  Assistant çš„å®Œç¾æ€è€ƒè¿‡ç¨‹ (â˜…â˜…â˜… å…³é”®ä¿®æ”¹ï¼šä¸¥æ ¼å¯¹é½ SFT æ ¼å¼ â˜…â˜…â˜…)
    # å¿…é¡»åŒ…å« "Since... I choose to..." è¿™ä¸€å¥ï¼Œè¿™æ˜¯è§¦å‘æ­£ç¡®å†³ç­–çš„å¼€å…³
    
    # æ³¨æ„ï¼šSFTæ•°æ®é‡Œçš„ä»£ç æ˜¯ print('Sure:', ... 'Gamble:', ...)
    # æˆ‘ä»¬è¿™é‡Œå°½é‡è¿˜åŸé‚£ä¸ª print çš„æ ¼å¼
    code_snippet = f"print('Sure:', {sure}**{TARGET_ALPHA}, 'Gamble:', {p1}/100 * ({v1}**{TARGET_ALPHA}))"
    tool_output = f"Sure: {val_sure:.5f} Gamble: {val_gamble:.5f}"
    
    # æ¯”è¾ƒç¬¦å·
    comp_sign = ">" if val_sure > val_gamble else "<"
    
    assistant_prefix = f"""<think>
I need to compare the utility of the sure outcome with the expected utility of the prospect.
Alpha={TARGET_ALPHA}, Lambda={TARGET_LAMBDA}.
</think>
<tool>{code_snippet}</tool>
<tool_output>{tool_output}</tool_output>
<think>
Comparing: {val_sure:.5f} vs {val_gamble:.5f}
Since {val_sure:.5f} {comp_sign} {val_gamble:.5f}, I choose to {decision_text}.
</think>
Final Decision: """ # â˜…â˜…â˜… æ³¨æ„ï¼šè¿™é‡ŒåŠ äº†ä¸€ä¸ªç©ºæ ¼ï¼SFTæ•°æ®é‡Œå†’å·åæœ‰ç©ºæ ¼

    return prompt_text + assistant_prefix

# ============================
# 3. æ ¸å¿ƒè¯„æµ‹å‡½æ•°
# ============================
def evaluate_model(model, tokenizer, samples, model_name):
    print(f"\nğŸ§ª Evaluating {model_name}...")
    
    # è·å– accept/reject çš„ token id
    # æ³¨æ„ï¼šQwen çš„ accept å‰é¢é€šå¸¸å¸¦ç©ºæ ¼
    id_accept = tokenizer.encode(" accept")[0]
    id_reject = tokenizer.encode(" reject")[0]
    
    results = []
    
    for sample in tqdm(samples):
        p1, v1, v2, sure = sample
        
        # æ„é€ å®Œç¾ä¸Šä¸‹æ–‡
        context = construct_perfect_context(tokenizer, p1, v1, v2, sure)
        inputs = tokenizer([context], return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        # è·å–æœ€åä¸€ä¸ª token çš„ logits
        logits = outputs.logits[0, -1, :]
        
        score_accept = logits[id_accept].item()
        score_reject = logits[id_reject].item()
        
        # è®¡ç®— Logit Diff (Accept - Reject)
        logit_diff = score_accept - score_reject
        
        # è®¡ç®—çœŸå®çš„ Utility Diff (Sure - Gamble)
        # ç†è®ºä¸Šï¼šUtility Diff > 0 (Sureå¥½) -> Logit Diff > 0 (Acceptå¥½)
        # ä¸¤è€…åº”è¯¥æ­£ç›¸å…³
        util_diff = get_utility_diff(p1, v1, v2, sure)
        
        results.append({
            "model": model_name,
            "utility_diff": util_diff,
            "logit_diff": logit_diff
        })
        
    return pd.DataFrame(results)

# ============================
# 4. ä¸»æ‰§è¡Œæµç¨‹
# ============================
if __name__ == "__main__":
    # ç”Ÿæˆæµ‹è¯•é›† (å›ºå®šéšæœºç§å­ä»¥å…¬å¹³å¯¹æ¯”)
    np.random.seed(42)
    samples = []
    for _ in range(NUM_SAMPLES):
        v1 = np.random.randint(500, 3000)
        v2 = 0
        p1 = 50
        # sure åœ¨ 20% åˆ° 80% ä¹‹é—´æ³¢åŠ¨ï¼Œè¦†ç›– accept å’Œ reject çš„è¾¹ç•Œ
        sure = np.random.randint(int(v1*0.2), int(v1*0.8))
        samples.append((p1, v1, v2, sure))

    # --- Round 1: æµ‹è¯• Base Model ---
    print("ğŸ”„ Loading Base Model...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, device_map="auto", torch_dtype=torch.bfloat16)
    
    df_base = evaluate_model(model, tokenizer, samples, "Base Model (Prompted)")
    
    # æ¸…ç†æ˜¾å­˜
    del model
    gc.collect()
    torch.cuda.empty_cache()

    # --- Round 2: æµ‹è¯• SFT Model ---
    print("ğŸ”„ Loading SFT Model...")
    # å…ˆåŠ è½½åŸºåº§
    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, device_map="auto", torch_dtype=torch.bfloat16)
    # å†åŠ è½½ Adapter
    model = PeftModel.from_pretrained(model, SFT_MODEL_PATH)
    
    df_sft = evaluate_model(model, tokenizer, samples, "SFT Model (Trained)")
    
    # --- Round 3: ç»˜å›¾å¯¹æ¯” ---
    print("\nğŸ“Š Plotting results...")
    df_all = pd.concat([df_base, df_sft])
    
    plt.figure(figsize=(12, 6))
    
    # ç»˜åˆ¶æ•£ç‚¹å›¾å’Œå›å½’çº¿
    sns.scatterplot(data=df_all, x="utility_diff", y="logit_diff", hue="model", alpha=0.6)
    sns.regplot(data=df_base, x="utility_diff", y="logit_diff", scatter=False, color="blue", label="Base Trend")
    sns.regplot(data=df_sft, x="utility_diff", y="logit_diff", scatter=False, color="orange", label="SFT Trend")
    
    plt.axvline(0, color='gray', linestyle='--', alpha=0.5)
    plt.axhline(0, color='gray', linestyle='--', alpha=0.5)
    
    plt.title("Logits vs. Utility Alignment: Base vs. SFT")
    plt.xlabel("True Utility Difference (U_sure - EU_gamble)")
    plt.ylabel("Model Logit Difference (Logit_accept - Logit_reject)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.savefig("comparison_logits_utility.png")
    print("ğŸ“ˆ Chart saved to 'comparison_logits_utility.png'")
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    corr_base = df_base['utility_diff'].corr(df_base['logit_diff'])
    corr_sft = df_sft['utility_diff'].corr(df_sft['logit_diff'])
    
    print("\n" + "="*40)
    print("ğŸ† CORRELATION RESULTS (Pearson r)")
    print("="*40)
    print(f"Base Model: {corr_base:.4f}")
    print(f"SFT Model:  {corr_sft:.4f}")
    print("-" * 40)
    
    if corr_sft > corr_base:
        print("âœ… Conclusion: SFT significantly improved alignment with the utility function.")
    else:
        print("ğŸ¤” Conclusion: Base model was already quite rational (or prompt was very effective).")