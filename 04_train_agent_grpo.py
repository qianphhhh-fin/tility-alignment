import re
import torch
from datasets import load_from_disk
from trl import GRPOConfig, GRPOTrainer
from transformers import AutoTokenizer
from peft import AutoPeftModelForCausalLM

# å¼•å…¥æˆ‘ä»¬çš„ç¯å¢ƒ Client
from calculator_env.client import CalculatorEnv
from calculator_env.models import CalculatorAction

# ============================
# é…ç½®
# ============================
SFT_MODEL_PATH = "sft-agent-0.6b"
OUTPUT_DIR = "grpo-agent-aligned"
ENV_URL = "http://localhost:8000" # ç¡®ä¿ server.app æ­£åœ¨è¿è¡Œ

# ============================
# è‡ªå®šä¹‰ Rollout å‡½æ•° (TRLçš„æ ¸å¿ƒ)
# ============================
def rollout_func(prompts, trainer, **kwargs):
    # è¿æ¥æœ¬åœ°ç¯å¢ƒ
    client = CalculatorEnv(base_url=ENV_URL)
    tokenizer = trainer.processing_class
    
    all_prompt_ids = []
    all_completion_ids = []
    all_logprobs = []
    
    # éå† Batch ä¸­çš„æ¯ä¸€æ¡ Prompt
    for i, prompt in enumerate(prompts):
        # 1. è§£æé¢˜ç›®å‚æ•°ï¼ŒåŒæ­¥ç»™ç¯å¢ƒ
        # å‡è®¾ dataset é‡Œçš„ prompt åŒ…å«äº†æ•°å­—ï¼Œæˆ‘ä»¬ç”¨æ­£åˆ™æå–
        # ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬æœ€å¥½ç›´æ¥ä» kwargs (dataset columns) é‡Œæ‹¿ï¼Œä½†åœ¨ rollout_func ç­¾åé‡Œ
        # TRL ç›®å‰ä¸»è¦ä¼  promptsã€‚æˆ‘ä»¬å°è¯•æ­£åˆ™è§£æ prompt æ–‡æœ¬ã€‚
        try:
            p1 = float(re.search(r"(\d+)% chance", prompt).group(1))
            vals = re.findall(r"\$(\d+)", prompt)
            v1, v2, sure = float(vals[0]), float(vals[1]), float(vals[2])
            client.set_problem(p1, v1, v2, sure)
            client.reset()
        except:
            pass # è§£æå¤±è´¥å°±ç”¨ç¯å¢ƒé»˜è®¤å€¼ï¼Œé˜²æ­¢ crash

        # 2. å¤šè½®äº¤äº’ç”Ÿæˆ
        current_text = prompt
        generated_text = ""
        
        for _ in range(4): # æœ€å¤šäº¤äº’ 4 è½®
            inputs = tokenizer(current_text, return_tensors="pt").to(trainer.model.device)
            
            with torch.no_grad():
                # ç”Ÿæˆæ¨¡å‹å›å¤
                outputs = trainer.model.generate(
                    **inputs,
                    max_new_tokens=256,
                    stop_strings=["</tool>"], # å…³é”®ï¼šé‡åˆ°æ ‡ç­¾åœæ­¢
                    tokenizer=tokenizer,
                    do_sample=True,
                    temperature=0.8
                )
            
            new_gen = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=False)
            current_text += new_gen
            generated_text += new_gen
            
            # å¤„ç†å·¥å…·è°ƒç”¨
            if "</tool>" in new_gen:
                # å‘é€ç»™ç¯å¢ƒ
                action = CalculatorAction(message=new_gen)
                step_res = client.step(action)
                
                # è·å–ç¯å¢ƒåé¦ˆ
                obs = step_res.observation.feedback # <tool_output>...</tool_output>
                current_text += obs
                generated_text += obs
            
            elif "Final Decision" in new_gen:
                break # ç»“æŸ
            else:
                break # å¼‚å¸¸ç»“æŸ

        # 3. æ•´ç†ç»“æœ
        # GRPO éœ€è¦ prompt_ids å’Œ completion_ids
        p_ids = tokenizer(prompt, add_special_tokens=False).input_ids
        c_ids = tokenizer(generated_text, add_special_tokens=False).input_ids
        
        all_prompt_ids.append(p_ids)
        all_completion_ids.append(c_ids)
        all_logprobs.append([0.0]*len(c_ids)) # å ä½

    return {
        "prompt_ids": all_prompt_ids,
        "completion_ids": all_completion_ids,
        "logprobs": all_logprobs
    }

# ============================
# å¥–åŠ±å‡½æ•°
# ============================
def reward_outcome(completions, **kwargs):
    """ç»“æœå¥–åŠ±ï¼šæ˜¯å¦åšå‡ºäº†ç¬¦åˆæ•°å­¦é¢„æœŸçš„å†³ç­–"""
    rewards = []
    for i, text in enumerate(completions):
        # è·å–çœŸå®å‚æ•°
        p1, v1, v2 = kwargs['p1'][i], kwargs['v1'][i], kwargs['v2'][i]
        sure = kwargs['sure'][i]
        alpha, lam = kwargs['alpha'][i], kwargs['lambda'][i]
        
        def u(x): return x**alpha if x>=0 else -lam*((-x)**alpha)
        
        u_sure = u(sure)
        u_gamble = (p1/100 * u(v1)) + ((100-p1)/100 * u(v2))
        optimal = "accept" if u_sure > u_gamble else "reject"
        
        decision = "unknown"
        if "Final Decision: accept" in text: decision = "accept"
        if "Final Decision: reject" in text: decision = "reject"
        
        if decision == optimal:
            rewards.append(1.0)
        elif decision == "unknown":
            rewards.append(-1.0)
        else:
            rewards.append(-0.5)
    return rewards

def reward_format(completions, **kwargs):
    """è¿‡ç¨‹å¥–åŠ±ï¼šæ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†å·¥å…·"""
    rewards = []
    for text in completions:
        score = 0.0
        if "<tool>" in text and "</tool>" in text:
            score += 0.2
        if "<tool_output>" in text: # è¯´æ˜æˆåŠŸè§¦å‘äº†ç¯å¢ƒåé¦ˆ
            score += 0.3
        rewards.append(score)
    return rewards

# ============================
# è®­ç»ƒä¸»æµç¨‹
# ============================
dataset = load_from_disk("my_local_agent_data") # åŠ è½½æœ¬åœ°ç”Ÿæˆçš„æ•°æ®

model = AutoPeftModelForCausalLM.from_pretrained(SFT_MODEL_PATH, is_trainable=True)
tokenizer = AutoTokenizer.from_pretrained(SFT_MODEL_PATH)

args = GRPOConfig(
    output_dir=OUTPUT_DIR,
    learning_rate=1e-6,
    num_generations=4,
    per_device_train_batch_size=2, # æ˜¾å­˜å°å°±è°ƒå°
    gradient_accumulation_steps=4,
    max_completion_length=1024,
    use_vllm=False, # ä½¿ç”¨è‡ªå®šä¹‰ rolloutï¼Œå…³é—­ vllm
    report_to="tensorboard"
)

trainer = GRPOTrainer(
    model=model,
    processing_class=tokenizer,
    reward_funcs=[reward_outcome, reward_format],
    train_dataset=dataset,
    args=args,
    rollout_func=rollout_func, # æ³¨å…¥è‡ªå®šä¹‰ Agent å¾ªç¯
)

print("ğŸš€ Starting Agentic GRPO Training...")
trainer.train()
trainer.save_model(OUTPUT_DIR)