import torch
import re
import json
import random
import numpy as np
from tqdm import tqdm
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

# å¼•å…¥ OpenEnv å®¢æˆ·ç«¯
from calculator_env.client import CalculatorEnv
from calculator_env.models import CalculatorAction

# ============================
# é…ç½®
# ============================
MODEL_PATH = "sft-agent-0.6b"
ENV_URL = "http://localhost:8000"
TEST_SAMPLES = 100  # æµ‹è¯•æ ·æœ¬æ•°é‡
LOG_FILE = "batch_test_results.jsonl"

# ç›®æ ‡äººæ ¼å‚æ•° (ç”¨äºè®¡ç®—æ ‡å‡†ç­”æ¡ˆ Ground Truth)
TARGET_ALPHA = 0.88
TARGET_LAMBDA = 2.25

# ============================
# è¾…åŠ©å‡½æ•°
# ============================
def calculate_ground_truth(p1, v1, v2, sure):
    """è®¡ç®—ç†æ€§å†³ç­–çš„æ ‡å‡†ç­”æ¡ˆ"""
    def u(x):
        if x >= 0: return x ** TARGET_ALPHA
        return -TARGET_LAMBDA * ((-x) ** TARGET_ALPHA)
    
    u_sure = u(sure)
    u_gamble = (p1/100 * u(v1)) + ((100-p1)/100 * u(v2))
    
    return "accept" if u_sure > u_gamble else "reject"

def generate_random_problem():
    """ç”Ÿæˆéšæœºæµ‹è¯•é¢˜"""
    p1 = 50
    v1 = random.randint(500, 3000)
    v2 = 0
    sure = random.randint(int(v1*0.2), int(v1*0.6)) # è¦†ç›– accept å’Œ reject çš„è¾¹ç•Œ
    return p1, v1, v2, sure

# ============================
# åˆå§‹åŒ–
# ============================
print(f"ğŸ”„ Loading Model: {MODEL_PATH}...")
model = AutoPeftModelForCausalLM.from_pretrained(
    MODEL_PATH, 
    device_map="auto", 
    torch_dtype=torch.bfloat16
).eval()
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

try:
    print(f"ğŸŒ Connecting to Environment at {ENV_URL}...")
    client = CalculatorEnv(base_url=ENV_URL)
    client.reset()
    print("âœ… Environment Connected!")
except Exception as e:
    print(f"âŒ Connection Failed: {e}")
    print("Please run 'python -m calculator_env.server.app' first.")
    exit()

# ============================
# å•æ¬¡æ¨ç†é€»è¾‘ (Agent Loop)
# ============================
def run_single_test(p1, v1, v2, sure):
    p2 = 100 - p1
    
    # 1. è®¾ç½®é¢˜ç›®
    client.set_problem(p1, v1, v2, sure)
    client.reset()
    
    # 2. æ„é€  Prompt
    system_prompt = "You are a rational economic agent. Use the <tool> tag to perform python calculations. Finally output 'Final Decision: accept' or 'reject'."
    user_prompt = f"The prospect is: {p1}% chance of ${v1}, {p2}% chance of ${v2}. The sure outcome is: ${sure}. Do you accept?"
    
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
    trajectory = text # è®°å½•å®Œæ•´å¯¹è¯å†å²
    tool_used = False
    tool_error = False
    decision = "unknown"
    
    # 3. å¤šè½®äº¤äº’å¾ªç¯
    for _ in range(4): # æœ€å¤šå…è®¸äº¤äº’ 4 è½®
        inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=200, 
                stop_strings=["</tool>"], # åªåœ¨å·¥å…·è°ƒç”¨ç»“æŸæ—¶åœï¼Œè®©å®ƒè‡ªå·±è¾“å‡º Final Decision
                tokenizer=tokenizer,
                do_sample=False # Greedy Decoding æµ‹èƒ½åŠ›è¾¹ç•Œ
            )
            
        new_content = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=False)
        text += new_content
        trajectory += new_content
        
        # A. æ£€æŸ¥æ˜¯å¦è°ƒç”¨å·¥å…·
        if "</tool>" in new_content:
            tool_used = True
            action = CalculatorAction(message=new_content)
            try:
                # è°ƒç”¨ OpenEnv
                step_result = client.step(action)
                feedback = step_result.observation.feedback
                
                # æ£€æŸ¥æ˜¯å¦æŠ¥é”™
                if "Error" in feedback:
                    tool_error = True
                
                # æ‹¼æ¥åˆ°ä¸Šä¸‹æ–‡
                text += feedback
                trajectory += feedback
                
            except Exception:
                tool_error = True
                break
        
        # B. æ£€æŸ¥æ˜¯å¦åšå‡ºå†³ç­–
        # æ³¨æ„ï¼šéœ€è¦å¤„ç†å¯èƒ½çš„é¢å¤–å­—ç¬¦ï¼Œç”¨æ­£åˆ™æå–
        if "Final Decision" in new_content:
            match = re.search(r"Final Decision:\s*(accept|reject)", new_content, re.IGNORECASE)
            if match:
                decision = match.group(1).lower()
            break
            
    return decision, tool_used, tool_error, trajectory

# ============================
# æ‰¹é‡æµ‹è¯•ä¸»æµç¨‹
# ============================
print(f"\nğŸš€ Starting Batch Evaluation ({TEST_SAMPLES} samples)...")
print(f"ğŸ“ Logs will be saved to: {LOG_FILE}")

stats = {
    "total": 0,
    "correct": 0,
    "tool_used_correctly": 0,
    "tool_errors": 0,
    "format_errors": 0
}

# æ¸…ç©ºæ—§æ—¥å¿—
open(LOG_FILE, 'w').close()

for i in tqdm(range(TEST_SAMPLES)):
    # 1. ç”Ÿæˆé¢˜ç›®
    p1, v1, v2, sure = generate_random_problem()
    ground_truth = calculate_ground_truth(p1, v1, v2, sure)
    
    # 2. è¿è¡Œ Agent
    model_decision, tool_used, tool_error, trace = run_single_test(p1, v1, v2, sure)
    
    # 3. ç»Ÿè®¡
    stats["total"] += 1
    is_correct = (model_decision == ground_truth)
    
    if is_correct:
        stats["correct"] += 1
    
    if tool_used and not tool_error:
        stats["tool_used_correctly"] += 1
        
    if tool_error:
        stats["tool_errors"] += 1
        
    if model_decision == "unknown":
        stats["format_errors"] += 1
        
    # 4. å®æ—¶è®°å½•æ—¥å¿—
    log_entry = {
        "id": i,
        "problem": {"p1": p1, "v1": v1, "v2": v2, "sure": sure},
        "ground_truth": ground_truth,
        "model_decision": model_decision,
        "is_correct": is_correct,
        "tool_used": tool_used,
        "tool_error": tool_error,
        "full_trace": trace
    }
    
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")

# ============================
# æœ€ç»ˆæŠ¥å‘Š
# ============================
print("\n" + "="*40)
print("ğŸ“Š EVALUATION REPORT")
print("="*40)
print(f"Total Samples:      {stats['total']}")
print(f"âœ… Accuracy:         {stats['correct'] / stats['total'] * 100:.2f}%")
print(f"ğŸ› ï¸ Tool Usage Rate:  {stats['tool_used_correctly'] / stats['total'] * 100:.2f}% (Valid Calls)")
print(f"âš ï¸ Tool Errors:      {stats['tool_errors']}")
print(f"âŒ Format Errors:    {stats['format_errors']} (No Decision)")
print("="*40)

if stats['correct'] / stats['total'] > 0.9:
    print("ğŸ‰ CONGRATULATIONS! Model is ready. No RL needed.")
else:
    print("ğŸ’ª Good start, but needs RL (Step 04) to improve robustness.")