import torch
from datasets import load_from_disk
from peft import LoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import SFTConfig, SFTTrainer

# ============================
# é…ç½®
# ============================
MODEL_NAME = "Qwen/Qwen3-0.6B" 
OUTPUT_DIR = "sft-agent-0.6b"
DATA_DIR = "my_local_agent_data"

# 1. åŠ è½½æ•°æ®
dataset = load_from_disk(DATA_DIR)

# 2. æ¨¡å‹ä¸åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

# 3. LoRA é…ç½®
peft_config = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    task_type="CAUSAL_LM", bias="none"
)

# 4. è®­ç»ƒå‚æ•°
training_args = SFTConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,             # 1ä¸ª epoch è¶³å¤Ÿå­¦ä¼šè¯­æ³•
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    max_length=1024,
    dataset_text_field="text",
    packing=False,
    bf16=True, # æ˜¾å¡æ”¯æŒå°±å¼€
)

# 5. è®­ç»ƒ
trainer = SFTTrainer(
    model=MODEL_NAME,
    train_dataset=dataset,
    peft_config=peft_config,
    processing_class=tokenizer,
    args=training_args,
)

print("ğŸš€ Starting SFT...")
trainer.train()
trainer.save_model(OUTPUT_DIR)
print(f"âœ… SFT Model saved to {OUTPUT_DIR}")